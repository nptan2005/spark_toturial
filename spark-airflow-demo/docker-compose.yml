# docker-compose-fullstack.yml (with logging mounts + json-file logging driver)
# Full Stack: Spark + Jupyter + Airflow + Kafka + MinIO + Governance + SSO
services:
  # ------------------- SPARK -------------------
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    platform: linux/amd64
    container_name: spark-master
    ports:
      - "8080:8080"   # Spark Master Web UI
      - "7077:7077"   # Spark Master port
    environment:
      - SPARK_MODE=master
      - SPARK_PUBLIC_DNS=spark-master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 15s
      timeout: 10s
      retries: 5
    volumes:
      - ../data:/data
      - ../logs/spark:/spark/logs
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - spark-net

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    platform: linux/amd64
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"   # Spark Worker Web UI
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 15s
      timeout: 10s
      retries: 5
    volumes:
      - ../data:/data
      - ../logs/spark:/spark/logs
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - spark-net

  # ------------------- JUPYTERLAB -------------------
  jupyterlab:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyterlab
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
    ports:
      - "8888:8888"  # JupyterLab Web UI
    volumes:
      - ./data:/home/jovyan/data
      - ../data:/data
      - ../logs/jupyter:/home/jovyan/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/realms/master"]
      interval: 20s
      timeout: 10s
      retries: 10
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - spark-net

  # ------------------- AIRFLOW INFRASTRUCTURE -------------------
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: ${AIRFLOW_POSTGRES_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_POSTGRES_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_POSTGRES_DB}
    expose:
      - "5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ../logs/postgres:/var/log/postgresql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_POSTGRES_USER}"]
      interval: 15s
      timeout: 10s
      retries: 5
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"
    networks:
      - spark-net

  redis:
    image: redis:latest
    container_name: airflow-redis
    mem_limit: ${REDIS_MEM}
    cpus: ${REDIS_CPU}
    ports:
      - "6379:6379"
    volumes:
      - ../logs/redis:/var/log/redis
    networks:
      - spark-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- AIRFLOW CORE SERVICES -------------------
  airflow-init:
    image: ${AIRFLOW_IMAGE_NAME}
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate && \
        airflow users create \
          --username ${_AIRFLOW_WWW_USER_USERNAME} \
          --password ${_AIRFLOW_WWW_USER_PASSWORD} \
          --firstname ${_AIRFLOW_WWW_USER_FIRSTNAME} \
          --lastname ${_AIRFLOW_WWW_USER_LASTNAME} \
          --role ${_AIRFLOW_WWW_USER_ROLE} \
          --email ${_AIRFLOW_WWW_USER_EMAIL} || true
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER}:${AIRFLOW_POSTGRES_PASSWORD}@postgres/${AIRFLOW_POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
    networks:
      - spark-net
    deploy:
      resources:
        limits:
          cpus: "${AIRFLOW_INIT_CPU}"
          memory: "${AIRFLOW_INIT_MEM}"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  airflow:
    image: ${AIRFLOW_IMAGE_NAME}
    container_name: ${AIRFLOW_CONTAINER_NAME}
    depends_on:
      postgres:
        condition: service_healthy
      spark-master:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER}:${AIRFLOW_POSTGRES_PASSWORD}@postgres/${AIRFLOW_POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__BASE_LOG_FOLDER=/opt/airflow/logs
      - AIRFLOW__LOGGING__BASE_LOG_FOLDER=/opt/airflow/logs
    expose: ["8080"]
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
      - ../data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - spark-net
    env_file:
      - .env
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  airflow-webserver:
    image: ${AIRFLOW_IMAGE_NAME}
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER}:${AIRFLOW_POSTGRES_PASSWORD}@postgres/${AIRFLOW_POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    command: webserver
    ports:
      - "8082:8080" # Airflow Web UI
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
      - ../data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - spark-net
    env_file:
      - .env
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  airflow-scheduler:
    image: ${AIRFLOW_IMAGE_NAME}
    container_name: airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER}:${AIRFLOW_POSTGRES_PASSWORD}@postgres/${AIRFLOW_POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
    command: scheduler
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
      - ../data:/data
    networks:
      - spark-net
    env_file:
      - .env
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  airflow-dag-processor:
    image: ${AIRFLOW_IMAGE_NAME}
    container_name: airflow-dag-processor
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER}:${AIRFLOW_POSTGRES_PASSWORD}@postgres/${AIRFLOW_POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR=True
    command: dag-processor
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type DagProcessorJob --hostname "$$${HOSTNAME}"',
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
      - ../data:/data
    networks:
      - spark-net
    env_file:
      - .env
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  flower:
    image: ${AIRFLOW_IMAGE_NAME}
    container_name: airflow-flower
    mem_limit: ${AIRFLOW_FLOWER_MEM}
    cpus: ${AIRFLOW_FLOWER_CPU}
    command: celery flower
    networks:
      - spark-net
    ports:
      - 5555:5555
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- MINIO -------------------
  minio:
    image: minio/minio
    container_name: minio
    command: >
      sh -c "minio server /data --console-address ":9001" 2>&1 | tee /var/log/minio/minio.log"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio:/data
      - ../logs/minio:/var/log/minio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 15s
      timeout: 10s
      retries: 5
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- KAFKA / ZOOKEEPER -------------------
  zookeeper:
    image: zookeeper:3.7.1
    platform: linux/amd64
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    volumes:
      - ../logs/zookeeper:/opt/zookeeper/logs
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 20s
      timeout: 10s
      retries: 5
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  kafka:
    image: wurstmeister/kafka:2.13-2.8.1
    platform: linux/amd64
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    volumes:
      - ../logs/kafka:/opt/kafka_2.13-2.8.1/logs
    environment:
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "29092"]
      interval: 15s
      timeout: 10s
      retries: 5
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- KEYCLOAK (SSO) -------------------
  keycloak:
    image: quay.io/keycloak/keycloak:21.1.1
    platform: linux/amd64
    container_name: keycloak
    environment:
      - KEYCLOAK_ADMIN=${KEYCLOAK_ADMIN}
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD}
    ports:
      - "8085:8080"
    volumes:
      - ./keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json
      - ../logs/keycloak:/opt/keycloak/logs
    entrypoint:
      - "/opt/keycloak/bin/kc.sh"
      - "start-dev"
      - "--import-realm"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/realms/master"]
      interval: 20s
      timeout: 10s
      retries: 10
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- SPARK UI OAUTH2 PROXY -------------------
  spark-ui-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:latest
    platform: linux/amd64
    container_name: spark-ui-proxy
    environment:
      OAUTH2_PROXY_CLIENT_ID: spark-ui
      OAUTH2_PROXY_CLIENT_SECRET: demo-secret
      OAUTH2_PROXY_COOKIE_SECRET: randomsecret123
      OAUTH2_PROXY_PROVIDER: keycloak
      OAUTH2_PROXY_OIDC_ISSUER_URL: http://keycloak:8080/realms/spark-demo-realm
      OAUTH2_PROXY_REDIRECT_URL: http://localhost:8084/oauth2/callback
      OAUTH2_PROXY_UPSTREAMS: http://spark-master:8080
    ports:
      - "8084:4180"
    depends_on:
      keycloak:
        condition: service_healthy
      spark-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4180"]
      interval: 20s
      timeout: 10s
      retries: 5
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ------------------- RANGER & ATLAS -------------------
  ranger:
    build:
      context: ./images/ranger
    image: local/ranger:2.3.0
    platform: linux/amd64
    container_name: ranger
    environment:
      DB_FLAVOR: "postgres"
      SQL_CONNECTOR_JAR: "/usr/share/java/postgresql.jar"
      db_host: "postgres"
      db_user: "${AIRFLOW_POSTGRES_USER}"
      db_password: "${AIRFLOW_POSTGRES_PASSWORD}"
    ports:
      - "6080:6080"
    depends_on:
      - postgres
      - keycloak
    volumes:
      - ../data:/data
      - ../logs/ranger:/var/log/ranger
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  atlas:
    build:
      context: ./images/atlas
    image: local/atlas:2.3.0
    platform: linux/amd64
    container_name: atlas
    ports:
      - "21000:21000"
    depends_on:
      - postgres
      - keycloak
    volumes:
      - ../data:/data
      - ../logs/atlas:/var/log/atlas
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  token-service:
    build: ./images/token-service
    image: local/token-service:latest
    container_name: token-service
    environment:
      - PORT=${TOKEN_SERVICE_PORT}
    ports:
      - "5001:5000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 10s
    volumes:
      - ../data:/data
      - ../logs/token-service:/app/logs
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  access-host-proxy:
    build:
      context: ./nginx
    image: local/nginx-proxy:latest
    container_name: access-host-proxy
    ports:
      - "5432:5432"
      - "1521:1521"
    expose:
      - "8081"
    volumes:
      - ../logs/nginx:/var/log/nginx
    networks:
      - spark-net
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  nginx-prometheus-exporter:
    image: nginx/nginx-prometheus-exporter:0.10.0
    container_name: nginx-prometheus-exporter
    command: ["-nginx.scrape-uri", "http://access-host-proxy:8081/nginx_status"]
    depends_on:
      - access-host-proxy
    networks:
      - spark-net
    ports:
      - "9113:9113"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  loki:
    image: grafana/loki:2.8.2
    container_name: loki
    user: "0"
    volumes:
      - ./data/loki:/loki
    command: -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  promtail:
    image: grafana/promtail:2.8.2
    container_name: promtail
    volumes:
      - ./monitoring/promtail-config.yml:/etc/promtail/config.yml:ro
      - ../logs:/logs:ro
      # Optional: if your Docker host stores container logs under /var/lib/docker/containers
      # and you want promtail to read raw container json logs, uncomment the next line.
      # - /var/lib/docker/containers:/var/lib/docker/containers:ro
    depends_on:
      - loki
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    image: grafana/grafana:9.5.0
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - loki
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  cadvisor:
    image: gcr.io/google-containers/cadvisor:latest
    container_name: cadvisor
    platform: linux/amd64
    restart: unless-stopped
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    expose:
      - "8080"
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  grafana-storage:

networks:
  spark-net:
    driver: bridge
