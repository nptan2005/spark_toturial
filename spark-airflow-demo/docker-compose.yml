# Full stack compose for local dev (Spark + Jupyter + Airflow + Kafka + MinIO + Ranger + Atlas + Keycloak + Observability)
services:
  ####################
  # SPARK
  ####################
  spark-master:
    image: local/spark:3.5.1-full
    container_name: spark-master
    ports:
      - "8080:8080"   # Spark Master Web UI
      - "7077:7077"   # Spark Master port
      - "18080:18080" # Spark History Server (sau dùng history server)
    # chạy trực tiếp Master bằng spark-class (foreground, không thoát)
    command:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.master.Master
      - --host
      - spark-master
      - --port
      - "7077"
      - --webui-port
      - "8080"
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/opt/spark/history
    volumes:
      - ../data:/data
      - ../logs/spark:/spark/logs
      - ../spark/jobs:/opt/spark/jobs
      - ./data/spark-history:/opt/spark/history
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/json/ || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "1.5"
    networks:
      - spark-net

  spark-worker:
    image: local/spark:3.5.1-full
    container_name: spark-worker
    depends_on:
      - spark-master
    # chạy trực tiếp Worker
    command:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.worker.Worker
      - spark://spark-master:7077
      - --webui-port
      - "8081"
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_DAEMON_MEMORY=512m
    ports:
      - "8081:8081"   # Spark Worker Web UI
    volumes:
      - ../data:/data
      - ../logs/spark:/spark/logs
      - ../spark/jobs:/opt/spark/jobs
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 4g
          cpus: "2.0"
    networks:
      - spark-net

  ####################
  # JUPYTERLAB
  ####################
  jupyterlab:
    # image: jupyter/pyspark-notebook:latest
    build:
      context: ./images/jupyter
    image: local/jupyter-spark:3.5.1
    container_name: jupyterlab
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
      - JUPYTER_TOKEN=admin         # <--- thêm dòng này (ví dụ)
    ports:
      - "8888:8888"
    command:
      [
        "jupyter",
        "lab",
        "--ip=0.0.0.0",
        "--no-browser",
        "--allow-root",
        "--ServerApp.token=admin",
        "--ServerApp.password=",
        "--ServerApp.allow_origin=*",
        "--ServerApp.allow_remote_access=True"
      ]
    volumes:
      - ./data:/home/jovyan/data
      - ../data:/data
      - ../logs/jupyter:/home/jovyan/logs
      - ../workspace:/workspace
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8888 || exit 1"]
      interval: 20s
      timeout: 10s
      retries: 10
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - spark-net

  ####################
  # AIRFLOW (POSTGRES + REDIS + AIRFLOW services)
  ####################
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: ${AIRFLOW_POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${AIRFLOW_POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${AIRFLOW_POSTGRES_DB:-airflow}
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ../logs/postgres:/var/log/postgresql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_POSTGRES_USER:-airflow}"]
      interval: 15s
      timeout: 10s
      retries: 5
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"
    networks:
      - spark-net

  redis:
    image: redis:latest
    container_name: airflow-redis
    ports:
      - "6379:6379"
    volumes:
      - ../logs/redis:/var/log/redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - spark-net

  airflow-init:
    image: ${AIRFLOW_IMAGE_NAME:-local/airflow:2.10.5-custom}
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate && \
        airflow users create \
          --username ${_AIRFLOW_WWW_USER_USERNAME:-admin} \
          --password ${_AIRFLOW_WWW_USER_PASSWORD:-admin} \
          --firstname ${_AIRFLOW_WWW_USER_FIRSTNAME:-Admin} \
          --lastname ${_AIRFLOW_WWW_USER_LASTNAME:-User} \
          --role ${_AIRFLOW_WWW_USER_ROLE:-Admin} \
          --email ${_AIRFLOW_WWW_USER_EMAIL:-admin@example.com} || true
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR:-LocalExecutor}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY:-dummy_fernet_key}
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
      - ../airflow_home/scripts:/opt/airflow/scripts
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  airflow-webserver:
    image: ${AIRFLOW_IMAGE_NAME:-local/airflow:2.10.5-custom}
    container_name: airflow-webserver
    depends_on:
      - postgres
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR:-LocalExecutor}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      # option: set luôn để backward compat
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY:-dummy_fernet_key}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - MS_TEAMS_WEBHOOK=${MS_TEAMS_WEBHOOK}
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_LOG_BUCKET=airflow-logs
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-admin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-admin123}
    command: webserver
    ports:
      - "8082:8080"
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
      - ../airflow_home/scripts:/opt/airflow/scripts
      - ../data:/data
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  airflow-scheduler:
    image: ${AIRFLOW_IMAGE_NAME:-local/airflow:2.10.5-custom}
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - airflow-init
      - redis
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR:-LocalExecutor}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY:-dummy_fernet_key}
    command: scheduler
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
      - ../airflow_home/scripts:/opt/airflow/scripts
      - ../data:/data
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  airflow-dag-processor:
    image: ${AIRFLOW_IMAGE_NAME:-local/airflow:2.10.5-custom}
    container_name: airflow-dag-processor
    depends_on:
      - postgres
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR:-LocalExecutor}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY:-dummy_fernet_key}
      - AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR=True
    command: dag-processor
    restart: always
    volumes:
      - ../airflow_home/dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../airflow_home/plugins:/opt/airflow/plugins
      - ../airflow_home/scripts:/opt/airflow/scripts
      - ../data:/data
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  flower:
    image: ${AIRFLOW_IMAGE_NAME:-local/airflow:2.10.5-custom}
    container_name: airflow-flower
    command: celery flower
    ports:
      - 5555:5555
    depends_on:
      - redis
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=${EXECUTOR:-LocalExecutor}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_POSTGRES_USER:-airflow}:${AIRFLOW_POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY:-dummy_fernet_key}
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  ####################
  # MINIO
  ####################
  minio:
    image: minio/minio
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-admin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-admin123}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio:/data
      - ../logs/minio:/var/log/minio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 15s
      timeout: 10s
      retries: 5
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  ####################
  # KAFKA + ZOOKEEPER
  ####################
  zookeeper:
    image: zookeeper:3.7.1
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    volumes:
      - ../logs/zookeeper:/opt/zookeeper/logs
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  kafka:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - ../logs/kafka:/opt/kafka_2.13-2.8.1/logs
    networks:
      - spark-net
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.0"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  kafka-client:
    build:
      context: ./images/kafka-client
    image: local/kafka-client:latest
    container_name: kafka-client
    depends_on:
      - kafka
    volumes:
      - ../logs/kafka_client:/app/logs
    command: ["python", "producer.py"]
    networks:
      - spark-net

  ####################
  # KEYCLOAK
  ####################
  keycloak:
    image: quay.io/keycloak/keycloak:21.1.1
    container_name: keycloak
    environment:
      - KEYCLOAK_ADMIN=${KEYCLOAK_ADMIN:-admin}
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD:-admin}
    ports:
      - "8085:8080"
    volumes:
      - ./keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json
      - ../logs/keycloak:/opt/keycloak/logs
    entrypoint:
      - "/opt/keycloak/bin/kc.sh"
      - "start-dev"
      - "--import-realm"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  ####################
  # OAUTH2-PROXY FOR SPARK UI
  ####################
  spark-ui-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:latest
    container_name: spark-ui-proxy
    environment:
      OAUTH2_PROXY_CLIENT_ID: spark-ui
      OAUTH2_PROXY_CLIENT_SECRET: demo-secret
      OAUTH2_PROXY_COOKIE_SECRET: randomsecret123
      OAUTH2_PROXY_PROVIDER: keycloak
      OAUTH2_PROXY_OIDC_ISSUER_URL: http://keycloak:8080/realms/spark-demo-realm
      OAUTH2_PROXY_REDIRECT_URL: http://localhost:8084/oauth2/callback
      OAUTH2_PROXY_UPSTREAMS: http://spark-master:8080
    ports:
      - "8084:4180"
    depends_on:
      - keycloak
      - spark-master
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:4180 || exit 1"]
      interval: 20s
      timeout: 10s
      retries: 5
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  ####################
  # RANGER & ATLAS
  ####################
  ranger:
    build:
      context: ./images/ranger
    image: local/ranger:2.3.0
    container_name: ranger
    environment:
      DB_FLAVOR: "postgres"
      SQL_CONNECTOR_JAR: "/usr/share/java/postgresql.jar"
      db_host: "postgres"
      db_user: "${AIRFLOW_POSTGRES_USER:-airflow}"
      db_password: "${AIRFLOW_POSTGRES_PASSWORD:-airflow}"
    ports:
      - "6080:6080"
    depends_on:
      - postgres
      - keycloak
    volumes:
      - ../data:/data
      - ../logs/ranger:/var/log/ranger
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  atlas:
    build:
      context: ./images/atlas
    image: local/atlas:2.3.0
    container_name: atlas
    ports:
      - "21000:21000"
    depends_on:
      - postgres
      - keycloak
    volumes:
      - ../data:/data
      - ../logs/atlas:/var/log/atlas
    networks:
      - spark-net
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:21000/ || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 6
      start_period: 30s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  ####################
  # ACCESS PROXY (nginx)
  ####################
  access-host-proxy:
    build:
      context: ./images/nginx
    image: local/nginx-proxy:latest
    container_name: access-host-proxy
    ports:
      - "5432:5432"
      - "1521:1521"
    expose:
      - "8081"
    volumes:
      - ../logs/nginx:/var/log/nginx
    networks:
      - spark-net
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  ####################
  # MONITORING
  ####################
  nginx-prometheus-exporter:
    image: nginx/nginx-prometheus-exporter:0.10.0
    container_name: nginx-prometheus-exporter
    command: ["-nginx.scrape-uri", "http://access-host-proxy:8081/nginx_status"]
    depends_on:
      - access-host-proxy
    ports:
      - "9113:9113"
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    depends_on:
      - access-host-proxy
    networks:
      - spark-net
    deploy:
      resources:
        limits:
          memory: 800m
          cpus: "0.8"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  loki:
    image: grafana/loki:2.8.2
    container_name: loki
    user: "0"
    volumes:
      - ./data/loki:/loki
    command: -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"
    networks:
      - spark-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3100/ready"]
      interval: 10s
      retries: 5
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  promtail:
    image: grafana/promtail:2.8.2
    container_name: promtail
    volumes:
      - ./monitoring/promtail.yml:/etc/promtail/config.yml:ro
      # - ../logs:/logs:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - loki
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    image: grafana/grafana:9.5.0
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - loki
    networks:
      - spark-net
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    container_name: cadvisor
    platform: linux/arm64     # hoặc linux/aarch64, tùy Docker
    restart: unless-stopped
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    expose:
      - "8080"
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  ####################
  # POSTGRES DWH
  ####################
  postgres-dwh:
    image: postgres:15
    container_name: postgres-dwh
    environment:
      POSTGRES_USER: dwh
      POSTGRES_PASSWORD: dwh123
      POSTGRES_DB: dwhdb
    ports:
      - "5433:5432"
    volumes:
      - ./data/postgres_dwh:/var/lib/postgresql/data
      - ./init/postgres_dwh:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dwh -d dwhdb -h 127.0.0.1 -p 5432"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - spark-net
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  grafana-storage:

networks:
  spark-net:
    driver: bridge