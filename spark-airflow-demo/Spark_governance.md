# üöÄ Full Demo Stack: Spark + Airflow + Governance + SSO
## 1Ô∏è‚É£ M·ª•c ti√™u setup

* Data processing / compute: Spark Master/Worker, Hadoop, Kafka
* Workflow orchestration: Airflow + DAG demo
* Storage: MinIO (Data Lake)
* Metadata DB: Postgres
* Notebook: JupyterLab + PySpark
* Governance & Masking: Apache Ranger + Apache Atlas
* Authentication: Keycloak (SSO cho Spark UI / Jupyter / Airflow)

## 2Ô∏è‚É£ Folder structure (Docker + Conda ready):

```arduino

spark-airflow-demo/
‚îú‚îÄ docker-compose.yml
‚îú‚îÄ dags/
‚îÇ   ‚îî‚îÄ demo_spark_dag.py
‚îú‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ demo_spark_notebook.ipynb
‚îú‚îÄ data/
‚îú‚îÄ logs/
‚îú‚îÄ plugins/
‚îú‚îÄ config/
‚îú‚îÄ conda_env.yml
‚îú‚îÄ keycloak/
‚îÇ   ‚îî‚îÄ realm-export.json

```

## 3Ô∏è‚É£ Docker Compose (docker-compose.yml)

```yaml
version: "3.9"

services:
  # ---------------- SPARK ----------------
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_PUBLIC_DNS=spark-master
    networks:
      - spark-net

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"
    networks:
      - spark-net

  # ---------------- KAFKA ----------------
  zookeeper:
    image: bitnami/zookeeper:3.8
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    networks:
      - spark-net

  kafka:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - spark-net

  # ---------------- MinIO ----------------
  minio:
    image: minio/minio
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=admin123
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio:/data
    networks:
      - spark-net

  # ---------------- Postgres ----------------
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - spark-net

  # ---------------- Airflow ----------------
  airflow:
    image: apache/airflow:2.10.5-python3.12
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=C1Cg8QaV6rUzSZlQ9OCAFHVv-IWMBQuSvnfcMfnuEAg=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    ports:
      - "8082:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    depends_on:
      - postgres
    networks:
      - spark-net

  # ---------------- JupyterLab ----------------
  jupyterlab:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyterlab
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/notebooks
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
    networks:
      - spark-net

  # ---------------- Ranger + Atlas ----------------
  ranger:
    image: apache/ranger:2.3.0
    container_name: ranger
    ports:
      - "6080:6080"
    networks:
      - spark-net

  atlas:
    image: apache/atlas:2.2.0
    container_name: atlas
    ports:
      - "21000:21000"
    networks:
      - spark-net

  # ---------------- Keycloak ----------------
  keycloak:
    image: quay.io/keycloak/keycloak:21.1.1
    container_name: keycloak
    command: start-dev
    environment:
      - KEYCLOAK_ADMIN=admin
      - KEYCLOAK_ADMIN_PASSWORD=admin
    ports:
      - "8083:8080"
    volumes:
      - ./keycloak:/opt/keycloak/data/import
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge

```

## 4Ô∏è‚É£ Demo DAG (Airflow)

dags/demo_spark_dag.py:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
from pyspark.sql import SparkSession

def spark_job():
    spark = SparkSession.builder \
        .appName("AirflowSparkDemo") \
        .master("spark://spark-master:7077") \
        .getOrCreate()
    data = [("Alice", 30), ("Bob", 25)]
    df = spark.createDataFrame(data, ["name", "age"])
    df.show()

with DAG(
    dag_id="demo_spark_dag",
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,
    catchup=False
) as dag:
    t1 = PythonOperator(
        task_id="run_spark_job",
        python_callable=spark_job
    )

```

## 5Ô∏è‚É£ Demo Notebook (JupyterLab)

notebooks/demo_spark_notebook.ipynb:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("DemoNotebook") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

data = [("John", 28), ("Jane", 32)]
columns = ["name", "age"]

df = spark.createDataFrame(data, columns)
df.show()
```

## 6Ô∏è‚É£ Conda environment (conda_env.yml):

```yaml
name: spark_env
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.12
  - jupyterlab
  - pyspark=3.3.0
  - pandas
  - numpy
  - matplotlib
  - airflow=2.10.5
  - psycopg2
  - requests
  - kafka-python

```
* Kh·ªüi t·∫°o:

```bash
conda env create -f conda_env.yml
conda activate spark_env
```

* Ch·∫°y Spark + Airflow + Jupyter tr·ª±c ti·∫øp:

```bash
# Spark master/worker
$SPARK_HOME/sbin/start-master.sh
$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077

# Airflow
airflow db init
airflow scheduler &
airflow webserver -p 8082

# Jupyter
jupyter lab
```

## 7Ô∏è‚É£ Truy c·∫≠p UI:

| Service         | URL                                              | Port  |
| --------------- | ------------------------------------------------ | ----- |
| Spark Master UI | [http://localhost:8080](http://localhost:8080)   | 8080  |
| Spark Worker UI | [http://localhost:8081](http://localhost:8081)   | 8081  |
| JupyterLab      | [http://localhost:8888](http://localhost:8888)   | 8888  |
| Airflow         | [http://localhost:8082](http://localhost:8082)   | 8080  |
| MinIO           | [http://localhost:9001](http://localhost:9001)   | 9001  |
| Ranger          | [http://localhost:6080](http://localhost:6080)   | 6080  |
| Atlas           | [http://localhost:21000](http://localhost:21000) | 21000 |
| Keycloak        | [http://localhost:8083](http://localhost:8083)   | 8080  |


## 8Ô∏è‚É£ Notes & Tips:

1. Ranger + Atlas:

Ranger d√πng policy ƒë·ªÉ mask d·ªØ li·ªáu, authorize c√°c user.

Atlas d√πng metadata lineage, tracking job, data catalog.

2. Keycloak:

B·∫°n c√≥ th·ªÉ t·∫°o realm v√† user, sau ƒë√≥ config Spark / Jupyter / Airflow d√πng OAuth2 login.

3. Local test:

Ch·∫°y docker compose up -d ‚Üí check logs ‚Üí test notebook ‚Üí trigger DAG.

4. Outside Docker:

Conda env + native install Java + Spark + Hadoop, ch·ªâ c·∫ßn thay ƒë·ªïi master=localhost:7077.

## 9Ô∏è‚É£ Keycloak Realm + User Config:

T·∫°o m·ªôt realm Keycloak, c√≥ s·∫µn user demo v√† client OAuth2 cho Spark, Jupyter, Airflow.

File: keycloak/realm-export.json

```json
{
  "realm": "spark-demo-realm",
  "enabled": true,
  "users": [
    {
      "username": "demo_user",
      "enabled": true,
      "emailVerified": true,
      "credentials": [
        {
          "type": "password",
          "value": "demo123",
          "temporary": false
        }
      ]
    }
  ],
  "clients": [
    {
      "clientId": "spark-ui",
      "enabled": true,
      "protocol": "openid-connect",
      "redirectUris": ["http://localhost:8080/*"],
      "publicClient": true,
      "directAccessGrantsEnabled": true
    },
    {
      "clientId": "jupyterlab",
      "enabled": true,
      "protocol": "openid-connect",
      "redirectUris": ["http://localhost:8888/*"],
      "publicClient": true,
      "directAccessGrantsEnabled": true
    },
    {
      "clientId": "airflow",
      "enabled": true,
      "protocol": "openid-connect",
      "redirectUris": ["http://localhost:8082/*"],
      "publicClient": true,
      "directAccessGrantsEnabled": true
    }
  ]
}
```

## 10 C·∫•u h√¨nh OAuth2 Spark UI:

Spark Master / Worker c√≥ th·ªÉ b·∫≠t authentication b·∫±ng OAuth2 Proxy ho·∫∑c config tr·ª±c ti·∫øp Ranger plugin.

C√°ch ƒë∆°n gi·∫£n cho demo:

1. C√†i th√™m Spark Ranger Plugin (n·∫øu mu·ªën enforce policy, nh∆∞ng demo c√≥ th·ªÉ b·ªè qua).

2. S·ª≠ d·ª•ng keycloak-proxy ƒë·ªÉ b·∫£o v·ªá UI:

```yaml
spark-ui-proxy:
  image: quay.io/oauth2-proxy/oauth2-proxy:latest
  container_name: spark-ui-proxy
  environment:
    - OAUTH2_PROXY_CLIENT_ID=spark-ui
    - OAUTH2_PROXY_CLIENT_SECRET=demo-secret
    - OAUTH2_PROXY_COOKIE_SECRET=randomsecret123
    - OAUTH2_PROXY_PROVIDER=keycloak
    - OAUTH2_PROXY_OIDC_ISSUER_URL=http://keycloak:8080/realms/spark-demo-realm
    - OAUTH2_PROXY_REDIRECT_URL=http://localhost:8084/oauth2/callback
    - OAUTH2_PROXY_UPSTREAMS=http://spark-master:8080
  ports:
    - "8084:4180"
  depends_on:
    - keycloak
    - spark-master
  networks:
    - spark-net
```

* Truy c·∫≠p Spark UI: http://localhost:8084 ‚Üí b·∫°n s·∫Ω th·∫•y login Keycloak.

## 11 C·∫•u h√¨nh OAuth2 JupyterLab

JupyterLab h·ªó tr·ª£ OAuth2 login b·∫±ng extension jupyter-server-oauth.
Demo nhanh:

1. C√†i ƒë·∫∑t extension trong container Jupyter:

```bash
docker exec -it jupyterlab pip install jupyter-server-oauth
```

2. Th√™m config jupyter_notebook_config.py:

```python
c.ServerApp.oauth2_provider_class = 'jupyter_server_oauth.providers.keycloak.KeycloakOAuthProvider'
c.KeycloakOAuthProvider.client_id = 'jupyterlab'
c.KeycloakOAuthProvider.client_secret = 'demo-secret'
c.KeycloakOAuthProvider.openid_url = 'http://keycloak:8080/realms/spark-demo-realm/.well-known/openid-configuration'
```

3. Restart JupyterLab ‚Üí truy c·∫≠p http://localhost:8888 ‚Üí login Keycloak.

## 12. C·∫•u h√¨nh OAuth2 Airflow:

Airflow h·ªó tr·ª£ OAuth2 login qua Flask AppBuilder:

1. C√†i ƒë·∫∑t package:

```bash
docker exec -it airflow pip install apache-airflow[oauth]
```

2. Trong airflow.cfg ho·∫∑c ENV:

```ini
[webserver]
rbac = True
authenticate = True
auth_backend = airflow.providers.oauth2.auth_backend.oauth_auth
```

3. ENV variables:

```yaml
AIRFLOW__WEBSERVER__OAUTH_PROVIDERS=[{'name':'keycloak','token_key':'access_token','icon':'fa-key','remote_app':{'client_id':'airflow','client_secret':'demo-secret','api_base_url':'http://keycloak:8080/realms/spark-demo-realm/protocol/openid-connect','access_token_url':'http://keycloak:8080/realms/spark-demo-realm/protocol/openid-connect/token','authorize_url':'http://keycloak:8080/realms/spark-demo-realm/protocol/openid-connect/auth','client_kwargs':{'scope':'openid profile email'}}}]
```

* Truy c·∫≠p http://localhost:8082 ‚Üí login Keycloak.

## 13. Bonus Tips:
* Ranger + Atlas c√≥ th·ªÉ d√πng demo user ƒë·ªÉ t·∫°o policy & metadata.
* Khi mu·ªën demo data masking / governance, d√πng Spark ƒë·ªçc t·ª´ MinIO ‚Üí Ranger plugin enforce ‚Üí Atlas track lineage.
* Keycloak d·ªÖ d√†ng m·ªü r·ªông: th√™m user, group, role ‚Üí mapping cho Spark/Airflow/Jupyter.


# b·∫£n Docker Compose ho√†n ch·ªânh demo full stack v·ªõi:

* Spark Master / Worker
* JupyterLab + PySpark Notebook
* Airflow (LocalExecutor)
* MinIO (Data Lake)
* Postgres (Airflow Metadata DB)
* Zookeeper + Kafka
* Keycloak (SSO)
* OAuth2 Proxy b·∫£o v·ªá Spark UI & JupyterLab
* Ranger + Atlas (Governance / Data Masking)

```yaml
# docker-compose.yml
# Demo full stack: Spark + Jupyter + Airflow + Kafka + MinIO + Governance (Ranger/Atlas) + Keycloak SSO
# ----------------------------------------------------------
services:
  # ------------------- SPARK -------------------
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_PUBLIC_DNS=spark-master
    ports:
      - "8080:8080"  # Spark UI
      - "7077:7077"  # Spark Master port
    networks:
      - spark-net

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"  # Spark Worker UI
    networks:
      - spark-net

  # ------------------- JUPYTERLAB -------------------
  jupyterlab:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyterlab
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
    ports:
      - "8888:8888"  # JupyterLab Web
    volumes:
      - ./data:/home/jovyan/data
    networks:
      - spark-net

  # ------------------- AIRFLOW -------------------
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - spark-net

  airflow:
    image: apache/airflow:2.10.5-python3.12
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=C1Cg8QaV6rUzSZlQ9OCAFHVv-IWMBQuSvnfcMfnuEAg=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    ports:
      - "8082:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    depends_on:
      - postgres
    networks:
      - spark-net

  # ------------------- MINIO -------------------
  minio:
    image: minio/minio
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio:/data
    networks:
      - spark-net

  # ------------------- KAFKA / ZOOKEEPER -------------------
  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    container_name: zookeeper
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    ports:
      - "2181:2181"
    networks:
      - spark-net

  kafka:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    networks:
      - spark-net

  # ------------------- KEYCLOAK (SSO) -------------------
  keycloak:
    image: quay.io/keycloak/keycloak:21.1.1
    container_name: keycloak
    command: start-dev
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
    ports:
      - "8085:8080"  # Keycloak Web UI
    networks:
      - spark-net
    volumes:
      - ./keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json
    entrypoint:
      - "/opt/keycloak/bin/kc.sh"
      - "start-dev"
      - "--import-realm"

  # ------------------- SPARK UI OAuth2 Proxy -------------------
  spark-ui-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:latest
    container_name: spark-ui-proxy
    environment:
      - OAUTH2_PROXY_CLIENT_ID=spark-ui
      - OAUTH2_PROXY_CLIENT_SECRET=demo-secret
      - OAUTH2_PROXY_COOKIE_SECRET=randomsecret123
      - OAUTH2_PROXY_PROVIDER=keycloak
      - OAUTH2_PROXY_OIDC_ISSUER_URL=http://keycloak:8080/realms/spark-demo-realm
      - OAUTH2_PROXY_REDIRECT_URL=http://localhost:8084/oauth2/callback
      - OAUTH2_PROXY_UPSTREAMS=http://spark-master:8080
    ports:
      - "8084:4180"
    depends_on:
      - keycloak
      - spark-master
    networks:
      - spark-net

  # ------------------- RANGER & ATLAS (Governance / Masking) -------------------
  ranger:
    image: apache/ranger:2.3.0
    container_name: ranger
    environment:
      - RANGER_ADMIN_PASSWORD=admin
    ports:
      - "6080:6080"
    networks:
      - spark-net

  atlas:
    image: apache/atlas:2.2.0
    container_name: atlas
    ports:
      - "21000:21000"
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge
```

## üîπ Gi·∫£i th√≠ch flow demo

* Spark Master/Worker: ch·∫°y cluster, Spark UI: http://localhost:8080 ‚Üí OAuth2 proxy: http://localhost:8084
* JupyterLab: http://localhost:8888 ‚Üí k·∫øt n·ªëi Spark cluster
* Airflow: http://localhost:8082 ‚Üí DAG qu·∫£n l√Ω Spark Job / Kafka / MinIO workflows
* MinIO: Data Lake ‚Üí Spark ƒë·ªçc/ghi d·ªØ li·ªáu
* Kafka + Zookeeper: message broker cho real-time demo
* Keycloak: qu·∫£n l√Ω user, login SSO cho Spark UI, JupyterLab, Airflow
* Ranger + Atlas: qu·∫£n l√Ω policy, data lineage, masking demo

## ‚úÖ S·ª≠ d·ª•ng nhanh:

```bash
docker compose pull
docker compose up -d
# Ki·ªÉm tra logs
docker compose logs -f keycloak
docker compose logs -f spark-master
```

* Spark UI qua OAuth2 Proxy: http://localhost:8084 ‚Üí login Keycloak: demo_user/demo123
* JupyterLab login Keycloak: http://localhost:8888
* Airflow login Keycloak: http://localhost:8082

# DEMO:

1. Notebook PySpark: ƒë·ªçc d·ªØ li·ªáu t·ª´ MinIO, th·ª±c hi·ªán Spark job (v√≠ d·ª• transform d·ªØ li·ªáu), ghi k·∫øt qu·∫£ l·∫°i v√†o MinIO.
2. DAG Airflow: qu·∫£n l√Ω job n√†y, g·ªìm task trigger, SparkSubmitOperator, sensor ki·ªÉm tra k·∫øt qu·∫£.
3. Comment chi ti·∫øt t·ª´ng b∆∞·ªõc flow, ƒë·ªÉ b·∫°n m·ªü l√™n l√† test ngay.

## 1Ô∏è‚É£ Th∆∞ m·ª•c demo
```text
spark-airflow-demo/
‚îÇ
‚îú‚îÄ dags/
‚îÇ   ‚îî‚îÄ spark_minio_demo_dag.py
‚îú‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ spark_minio_demo.ipynb
‚îú‚îÄ data/
‚îÇ   ‚îú‚îÄ input/       # d·ªØ li·ªáu g·ªëc MinIO
‚îÇ   ‚îî‚îÄ output/      # k·∫øt qu·∫£
‚îú‚îÄ docker-compose.yml

```
## 2Ô∏è‚É£ Notebook demo: notebooks/spark_minio_demo.ipynb:

MinIO (input CSV) ‚Üí Spark (transform) ‚Üí MinIO (output CSV)

```python
# PySpark + MinIO demo notebook

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# 1. Kh·ªüi t·∫°o SparkSession
spark = SparkSession.builder \
    .appName("Spark MinIO Demo") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# 2. C·∫•u h√¨nh k·∫øt n·ªëi MinIO (S3 API)
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "admin")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "admin123")
spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "http://minio:9000")
spark._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")

input_path = "s3a://demo-bucket/input/sample.csv"
output_path = "s3a://demo-bucket/output/result.csv"

# 3. ƒê·ªçc d·ªØ li·ªáu t·ª´ MinIO
df = spark.read.option("header", "true").csv(input_path)
print("Input Data:")
df.show()

# 4. Transform d·ªØ li·ªáu (v√≠ d·ª•: ch·ªçn c·ªôt, ƒë·ªïi t√™n, t√≠nh to√°n)
df_transformed = df.select(
    col("id"),
    col("value").cast("double"),
    (col("value").cast("double") * 2).alias("value_double")
)

# 5. Ghi k·∫øt qu·∫£ v·ªÅ MinIO
df_transformed.write.mode("overwrite").option("header", "true").csv(output_path)
print(f"Output saved to {output_path}")
```

## 3Ô∏è‚É£ DAG Airflow: dags/spark_minio_demo_dag.py:

Airflow DAG ‚Üí SparkSubmitOperator ‚Üí Sensor ki·ªÉm tra k·∫øt qu·∫£ MinIO ‚Üí ho√†n t·∫•t

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import PythonOperator
import boto3

# 1. H√†m ki·ªÉm tra file tr√™n MinIO
def check_minio_file():
    s3 = boto3.client(
        "s3",
        endpoint_url="http://minio:9000",
        aws_access_key_id="admin",
        aws_secret_access_key="admin123"
    )
    result = s3.list_objects_v2(Bucket="demo-bucket", Prefix="output/result.csv")
    if "Contents" not in result:
        raise ValueError("Result not found yet!")
    print("Result file found in MinIO.")

# 2. DAG definition
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=1),
}

dag = DAG(
    "spark_minio_demo",
    default_args=default_args,
    description="Demo DAG: Spark job + MinIO",
    schedule_interval=None,
    start_date=datetime(2025, 11, 10),
    catchup=False,
)

# 3. SparkSubmitOperator
spark_task = SparkSubmitOperator(
    task_id="spark_minio_task",
    application="/opt/airflow/dags/notebooks/spark_minio_demo.py",  # convert notebook -> py ho·∫∑c d√πng .py script
    name="spark_minio_demo",
    conn_id="spark_default",
    verbose=True,
    dag=dag,
)

# 4. PythonOperator: ki·ªÉm tra k·∫øt qu·∫£ tr√™n MinIO
check_task = PythonOperator(
    task_id="check_minio_result",
    python_callable=check_minio_file,
    dag=dag
)

spark_task >> check_task
```

**Notes**:
* DAG trigger b·∫±ng tay ho·∫∑c schedule theo gi·ªù.
* SparkSubmitOperator ch·∫°y script .py t·ª´ Airflow container. N·∫øu notebook .ipynb, b·∫°n convert sang .py (v√≠ d·ª• d√πng * jupyter nbconvert).
* check_minio_file ki·ªÉm tra k·∫øt qu·∫£ tr√™n MinIO bucket.

## 4Ô∏è‚É£ Test nhanh:

1. Start Docker full stack demo:

```bash
docker compose up -d
```

2. T·∫°o bucket v√† upload d·ªØ li·ªáu sample v√†o MinIO:

```bash
docker exec -it minio mc alias set local http://localhost:9000 admin admin123
docker exec -it minio mc mb local/demo-bucket
docker exec -it minio mc cp ./sample.csv local/demo-bucket/input/
```

3. Truy c·∫≠p Airflow UI: http://localhost:8082 ‚Üí trigger DAG spark_minio_demo.
4. Spark job s·∫Ω ch·∫°y, k·∫øt qu·∫£ l∆∞u MinIO ‚Üí PythonOperator ki·ªÉm tra file ‚Üí DAG ho√†n t·∫•t.
5. Ki·ªÉm tra MinIO output: http://localhost:9001 (login admin/admin123).

# Compose full stack v2:
* Spark Master / Worker
* MinIO
* Airflow + DAG + Notebook convert s·∫µn .py
* Postgres
* Kafka + Zookeeper
* Keycloak 21.1.1 + OAuth2 Proxy cho Spark/Jupyter
* Ranger / Atlas (version ·ªïn ƒë·ªãnh)

```yaml
# docker-compose-fullstack.yml
# ----------------------------------------------------------
# Demo Full Stack: Spark + Jupyter + Airflow + Kafka + MinIO
# + Governance (Ranger / Atlas)
# + Keycloak SSO + OAuth2 Proxy cho Spark UI
# ----------------------------------------------------------

version: "3.9"

services:
  # ------------------- SPARK -------------------
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_PUBLIC_DNS=spark-master
    ports:
      - "8080:8080"   # Spark Master Web UI
      - "7077:7077"   # Spark Master port
    networks:
      - spark-net

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"   # Spark Worker Web UI
    networks:
      - spark-net

  # ------------------- JUPYTERLAB -------------------
  jupyterlab:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyterlab
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
    ports:
      - "8888:8888"  # JupyterLab Web UI
    volumes:
      - ./data:/home/jovyan/data
    networks:
      - spark-net

  # ------------------- AIRFLOW + POSTGRES -------------------
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - spark-net

  airflow:
    image: apache/airflow:2.10.5-python3.12
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=C1Cg8QaV6rUzSZlQ9OCAFHVv-IWMBQuSvnfcMfnuEAg=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    ports:
      - "8082:8080"  # Airflow Web UI
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    depends_on:
      - postgres
    networks:
      - spark-net

  # ------------------- MINIO (Data Lake) -------------------
  minio:
    image: minio/minio
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    ports:
      - "9000:9000"  # MinIO API
      - "9001:9001"  # MinIO Console
    volumes:
      - ./data/minio:/data
    networks:
      - spark-net

  # ------------------- KAFKA / ZOOKEEPER -------------------
  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    container_name: zookeeper
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    ports:
      - "2181:2181"
    networks:
      - spark-net

  kafka:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    networks:
      - spark-net

  # ------------------- KEYCLOAK (SSO) -------------------
  keycloak:
    image: quay.io/keycloak/keycloak:21.1.1
    container_name: keycloak
    command: start-dev
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
    ports:
      - "8085:8080"  # Keycloak Web UI
    networks:
      - spark-net
    volumes:
      - ./keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json
    entrypoint:
      - "/opt/keycloak/bin/kc.sh"
      - "start-dev"
      - "--import-realm"

  # ------------------- SPARK UI OAuth2 Proxy -------------------
  spark-ui-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:latest
    container_name: spark-ui-proxy
    environment:
      OAUTH2_PROXY_CLIENT_ID: spark-ui
      OAUTH2_PROXY_CLIENT_SECRET: demo-secret
      OAUTH2_PROXY_COOKIE_SECRET: randomsecret123
      OAUTH2_PROXY_PROVIDER: keycloak
      OAUTH2_PROXY_OIDC_ISSUER_URL: http://keycloak:8080/realms/spark-demo-realm
      OAUTH2_PROXY_REDIRECT_URL: http://localhost:8084/oauth2/callback
      OAUTH2_PROXY_UPSTREAMS: http://spark-master:8080
    ports:
      - "8084:4180"  # OAuth2 Proxy for Spark UI
    depends_on:
      - keycloak
      - spark-master
    networks:
      - spark-net

  # ------------------- RANGER & ATLAS -------------------
  ranger:
    image: sabrinapark/apache-ranger:2.3.0
    container_name: ranger
    environment:
      RANGER_ADMIN_PASSWORD: admin
    ports:
      - "6080:6080"  # Ranger Admin UI
    networks:
      - spark-net

  atlas:
    image: sburn/apache-atlas:2.3.0
    container_name: atlas
    ports:
      - "21000:21000"
    networks:
      - spark-net
    depends_on:
      - postgres

# ------------------- NETWORK -------------------
networks:
  spark-net:
    driver: bridge
```

# Demo flow: MinIO ‚Üí Spark Job ‚Üí Result ‚Üí Airflow Track:

## A. Notebook PySpark: notebooks/spark_minio_demo.ipynb

(·ªü ƒë√¢y l√† phi√™n b·∫£n .py b·∫°n c√≥ th·ªÉ convert d·ªÖ d√†ng)

```python
# spark_minio_demo.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

def main():
    spark = SparkSession.builder \
        .appName("Spark MinIO Demo") \
        .master("spark://spark-master:7077") \
        .getOrCreate()

    # c·∫•u h√¨nh MinIO (S3 API)
    spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "admin")
    spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "admin123")
    spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "http://minio:9000")
    spark._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")

    input_path = "s3a://demo-bucket/input/sample.csv"
    output_path = "s3a://demo-bucket/output/result"

    df = spark.read.option("header", "true").csv(input_path)
    print("== Input Data ==")
    df.show()

    # transform
    df2 = df.select(
        col("id"),
        col("value").cast("double").alias("value_num"),
        (col("value").cast("double") * 2).alias("value_double")
    )

    print("== Transformed Data ==")
    df2.show()

    # write back
    df2.write.mode("overwrite").option("header", "true").csv(output_path)
    print(f"Output saved to {output_path}")

    spark.stop()

if __name__ == "__main__":
    main()
```

## B. DAG Airflow: dags/spark_minio_demo_dag.py:

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import PythonOperator
import boto3

def check_minio_file():
    s3 = boto3.client(
        "s3",
        endpoint_url="http://minio:9000",
        aws_access_key_id="admin",
        aws_secret_access_key="admin123"
    )
    resp = s3.list_objects_v2(Bucket="demo-bucket", Prefix="output/")
    if "Contents" not in resp or len(resp["Contents"]) == 0:
        raise ValueError("Result file not found in MinIO")
    print("Result file exists in MinIO")

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=1),
}

with DAG(
    "spark_minio_demo",
    default_args=default_args,
    description="Demo Spark job via Airflow, input from MinIO, output to MinIO",
    schedule_interval=None,
    start_date=datetime(2025,1,1),
    catchup=False
) as dag:

    spark_task = SparkSubmitOperator(
        task_id="spark_task",
        application="/opt/airflow/dags/spark_minio_demo.py",
        name="spark_minio_demo",
        conn_id="spark_default",
        verbose=True
    )

    check_task = PythonOperator(
        task_id="check_minio",
        python_callable=check_minio_file
    )

    spark_task >> check_task
```

## C. File c·∫•u h√¨nh docker‚Äëcompose.yml (c√≥ s·∫µn, ch·ªâ c·∫ßn ƒë·∫£m b·∫£o volumes v√† paths ƒë√∫ng)

B·∫°n ƒë√£ c√≥ ph·∫ßn l·ªõn. B·∫°n c·∫ßn ƒë·∫£m b·∫£o:

* T·∫°o bucket demo-bucket trong MinIO tr∆∞·ªõc khi ch·∫°y DAG
* Copy file sample.csv v√†o MinIO path input/sample.csv
* Volume ./notebooks ch·ª©a script .py v√† notebook n·∫øu c·∫ßn
* Volume ./dags ch·ª©a DAG v√† script .py

## üîç 3. H∆∞·ªõng d·∫´n v·∫≠n h√†nh nhanh:

1. Start to√†n b·ªô stack:

```bash
docker compose up -d
```

2. Truy c·∫≠p MinIO console: http://localhost:9001, login admin/admin123.
T·∫°o bucket demo-bucket. Upload sample.csv v√†o input/.

3. Truy c·∫≠p Airflow UI: http://localhost:8082, user n·∫øu ch∆∞a SSO th√¨ m·∫∑c ƒë·ªãnh.
C√≥ th·∫•y DAG spark_minio_demo. Click Trigger.

4. Job ch·∫°y: Spark cluster s·∫Ω th·ª±c hi·ªán transform.
Sau ƒë√≥ PythonOperator ki·ªÉm tra MinIO ra output/.

5. Truy c·∫≠p http://localhost:9001 l·∫°i, ki·ªÉm tra bucket demo-bucket/output/ c√≥ th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£.

6. B·∫°n c√≥ th·ªÉ m·ªü JupyterLab http://localhost:8888, load notebook ho·∫∑c script v√† ch∆°i th·ª≠.


# 1Ô∏è‚É£ Build image Atlas / Ranger local:

## Atlas:

```bash
# 1. Clone source Atlas
git clone https://github.com/apache/atlas.git
cd atlas
# Checkout version b·∫°n mu·ªën (v√≠ d·ª• 2.2.0)
git checkout rel/2.2.0

# error: tiep
git fetch --all --tags

# Xem danh s√°ch tag
git tag -l

# V√≠ d·ª•, tag 2.2.0 c√≥ t√™n 'apache-atlas-2.2.0'
git checkout release-2.2.0-rc0  
# 2. T·∫°o docker file trong folder: spark-airflow-demo\images\atlas> n·ªôi dung nh∆∞ b√™n d∆∞·ªõi:
Download v√† gi·∫£i n√©n file atlas t·ª´ apache
# 3. Build Docker image
docker build -t local/atlas:2.2.0 .
```
* Docker file
```Dockerfile
# ===============================
# Apache Atlas 2.3.0 - Dockerfile
# ===============================

# Dockerfile for Apache Atlas (cross-platform)
FROM eclipse-temurin:11-jdk
WORKDIR /opt/atlas

# Copy Atlas binaries v√†o container
COPY apache-atlas-2.3.0 /opt/atlas

EXPOSE 21000

CMD ["./bin/atlas_start.sh"]

```

* B∆∞·ªõc 2: T·∫°o file entrypoint.sh c√πng th∆∞ m·ª•c:

```bash
#!/bin/bash
echo "Starting Apache Atlas Server..."
$ATLAS_HOME/bin/atlas_start.py

# Gi·ªØ container ch·∫°y ƒë·ªÉ b·∫°n c√≥ th·ªÉ attach logs
tail -f $ATLAS_HOME/logs/atlas.log

```

* üëâ B∆∞·ªõc 3: Build l·∫°i image:

```bash
docker build -t local/atlas:2.3.0 . 
```

* L∆∞u √Ω: Apache Atlas c√≥ y√™u c·∫ßu Java + Maven, build image s·∫Ω m·∫•t v√†i ph√∫t.

## Ranger:
T∆∞∆°ng t·ª± atlas c≈©ng c·∫ßn download source >>> docker file v√† build
```Dockerfile
# Dockerfile: Ranger 2.3.0 binary (cross-platform)
FROM eclipse-temurin:11-jdk
WORKDIR /opt/ranger

# Copy Ranger binary v√†o container
COPY apache-ranger-2.3.0 /opt/ranger

# Expose Ranger Admin port
EXPOSE 6080

# Start Ranger Admin khi container run
CMD ["./bin/ranger-admin-start.sh"]
```

```bash
docker build -t local/ranger:2.3.0 .
```


```bash
# 1. Clone source Ranger
git clone https://github.com/apache/ranger.git
cd ranger
# Checkout version 2.3.0
git checkout rel/2.3.0

# 2. Build Docker image
docker build -t local/ranger:2.3.0 .
```

* Ranger c≈©ng c·∫ßn Java + Maven, build image xong b·∫°n s·∫Ω c√≥ image local.

## Tokenization Service:

1. C·∫•u tr√∫c:

```css
images/token-service/
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ app.py
```

2. Dockerfile token-service
   
```Dockerfile
# Dockerfile token-service
FROM python:3.12-slim

WORKDIR /app

# Copy code v√†o
COPY app.py /app

# C√†i Flask
RUN pip install flask

# Expose port
EXPOSE 5000

# Start service
CMD ["python", "app.py"]
```

3. app.py

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

# Simple token store
store = {}

@app.route("/token", methods=["POST"])
def generate_token():
    data = request.json
    val = data.get("value")
    if not val:
        return jsonify({"error": "missing value"}), 400
    token = f"TOKEN-{len(store)+1}"
    store[token] = val
    return jsonify({"token": token})

@app.route("/token/<token>", methods=["GET"])
def get_token(token):
    val = store.get(token)
    if not val:
        return jsonify({"error": "token not found"}), 404
    return jsonify({"value": val})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
```

4. Build:

```bash
cd images/token-service
docker build -t local/token-service:latest .
```
* Ki·ªÉm tra:

```bash
cd images/token-service
docker build -t local/token-service:latest .
```

5. C·∫≠p nh·∫≠t Docker Compose:

```yaml
token-service:
  build: ./images/token-service
  image: local/token-service:latest
  container_name: token-service
  ports:
    - "5001:5000"
  networks:
    - spark-net
```

6. Test:

```bash
# T·∫°o token
curl -X POST -H "Content-Type: application/json" -d '{"value":"mydata"}' http://localhost:5001/token

# L·∫•y gi√° tr·ªã token
curl http://localhost:5001/token/TOKEN-1
```

## 2Ô∏è‚É£ S·ª≠a docker-compose.yml:

* D√πng image local:

```yaml
atlas:
  image: local/atlas:2.2.0
ranger:
  image: local/ranger:2.3.0
```

* Keycloak v·∫´n d√πng public image:

```yaml
keycloak:
  image: quay.io/keycloak/keycloak:21.1.1
```

* Zookeeper: zookeeper:3.7.1 (pull ƒë∆∞·ª£c)
* Kafka, Spark, Jupyter, MinIO, Postgres, Airflow: gi·ªØ nguy√™n.

## Build custom Spark:
```bash
cd images/spark
docker build --platform=linux/arm64 -t local/spark:3.5.1-full .
```

Test nhanh:

```bash
# ch·∫°y container interactive
docker run --rm -it --entrypoint /bin/zsh local/spark:3.5.1-full -l
# ho·∫∑c exec v√†o running container
docker exec -it spark-master /bin/zsh -l
```

## Airflow custom image:
```bash
docker build --platform=linux/arm64 -t local/airflow:2.10.5-custom .
```

### üîç Ki·ªÉm tra Airflow image
#### ‚úî Ki·ªÉm tra Airflow version
```bash
docker run --rm local/airflow:2.10.5-custom airflow version
```
#### ‚úî Ki·ªÉm tra cx_Oracle load Instant Client ƒë√∫ng
```bash
docker run -it --rm local/airflow:2.10.5-custom python3 - <<EOF
import cx_Oracle
print("cx_Oracle OK")
EOF
```
N·∫øu l·ªói DPI-1047, nghƒ©a l√† Instant Client ƒë·∫∑t sai ƒë∆∞·ªùng d·∫´n.

#### ‚úî Ki·ªÉm tra ORACLE_HOME & LD_LIBRARY_PATH
```bash
docker run --rm local/airflow:2.10.5-custom sh -c "echo ORACLE_HOME=$ORACLE_HOME; echo LD_LIBRARY_PATH=$LD_LIBRARY_PATH"
```
### üüß L·ªánh ki·ªÉm tra DAG import pyspark
(DAG kh√¥ng ch·∫°y Spark job, ch·ªâ ki·ªÉm tra import)
```bash
docker run -it --rm local/airflow:2.10.5-custom python3 - <<EOF
import pyspark
print("pyspark import OK")
EOF
```
### üü™ Ki·ªÉm tra Instant Client Oracle

```bash
docker run -it --rm local/airflow:2.10.5-custom ls /opt/oracle
docker run -it --rm local/airflow:2.10.5-custom ls /opt/oracle/instantclient_23_6
```

### üü• Ki·ªÉm tra Airflow Providers ƒë√£ c√†i

```bash
docker run -it --rm local/airflow:2.10.5-custom pip show apache-airflow-providers-oracle
docker run -it --rm local/airflow:2.10.5-custom pip show apache-airflow-providers-postgres
docker run -it --rm local/airflow:2.10.5-custom pip show apache-airflow-providers-docker
```

### üü¶ Ki·ªÉm tra SparkSubmitOperator t·ª´ Airflow container
```bash
docker run -it --rm local/airflow:2.10.5-custom pyspark --version
```
### üü© Ki·ªÉm tra DAG folder ƒë√£ mount ch√≠nh x√°c khi ch·∫°y docker-compose
sau khi b·∫°n ch·∫°y compose:
```bash
docker exec -it airflow-webserver ls /opt/airflow/dags
docker exec -it airflow-webserver airflow dags list
```
## Build Kafka client:
```bash
docker build --platform=linux/arm64 -t local/kafka-client:latest .
```
Test
```bash
docker run --rm \
  --network spark-net \
  -v $(pwd)/logs:/app/logs \
  local/kafka-client:latest
```

## 3Ô∏è‚É£ Flow full lu·ªìng:

1. MinIO: l∆∞u input d·ªØ li·ªáu CSV / Parquet.
2. Spark job (submit t·ª´ notebook ho·∫∑c Airflow DAG):
* ƒë·ªçc d·ªØ li·ªáu MinIO,
* x·ª≠ l√Ω t√≠nh to√°n,
* l∆∞u k·∫øt qu·∫£ v·ªÅ MinIO.
3. Airflow DAG:
* Trigger job Spark,
* Sensor check file result trong MinIO,
* Log k·∫øt qu·∫£ / tracking.
4. Ranger / Atlas:
* Ranger qu·∫£n l√Ω quy·ªÅn truy c·∫≠p data,
* Atlas qu·∫£n l√Ω metadata, lineage.
5. Keycloak + OAuth2 Proxy:
* SSO cho Spark UI / Jupyter / Airflow.

## üí° T√≥m t·∫Øt:
* Kh√¥ng c√≥ image public cho Atlas / Ranger, ph·∫£i build local.
* Keycloak public image c√≤n d√πng ƒë∆∞·ª£c.
* Khi build xong, docker-compose s·∫Ω ch·∫°y full stack.

```yaml
access-host-proxy:
    image: nginx:latest
    container_name: access-host-proxy
    mem_limit: ${NGINX_MEM}
    cpus: ${NGINX_CPU}
    # REMOVED: network_mode: host
    ports:
      - "5432:5432" # Expose PostgreSQL proxy port
      - "1521:1521" # Expose Oracle proxy port
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl
    networks:
      - spark-net # Quan tr·ªçng: Ph·∫£i thu·ªôc m·∫°ng chung ƒë·ªÉ d√πng t√™n d·ªãch v·ª•
    depends_on:
      postgres:
        condition: service_healthy
    restart: always
```
### B. D·ªãch v·ª• Airflow (`flower`, `redis`, `dag-processor`)

| D·ªãch v·ª• | V·∫•n ƒë·ªÅ/ƒêi·ªÅu ch·ªânh | L√Ω do |
| :--- | :--- | :--- |
| **Network Name** | D·ªãch v·ª• ƒëang s·ª≠ d·ª•ng `airflow-network`. | **Ph·∫£i h·ª£p nh·∫•t!** T·∫•t c·∫£ c√°c d·ªãch v·ª• (Spark, Kafka, MinIO, Airflow) n√™n s·ª≠ d·ª•ng m·ªôt m·∫°ng chung, v√≠ d·ª•: **`spark-net`**, ƒë·ªÉ Airflow c√≥ th·ªÉ g·ªçi Spark Submit ho·∫∑c Kafka Producer/Consumer. |
| **`flower`** | C·∫•u h√¨nh t·ªët, s·ª≠ d·ª•ng `service_completed_successfully` cho `airflow-init`. | ƒê·∫£m b·∫£o ƒë·ªïi `airflow-network` th√†nh `spark-net`. |
| **`redis`** | C·∫•u h√¨nh t·ªët. | ƒê·∫£m b·∫£o ƒë·ªïi `airflow-network` th√†nh `spark-net`. |
| **`airflow-dag-processor`** | C·∫ßn ƒë·∫£m b·∫£o r·∫±ng `airflow-common-depends-on` bao g·ªìm c·∫£ `redis` v√† `postgres` healthy tr∆∞·ªõc khi kh·ªüi ƒë·ªông. | `dag-processor` c·∫ßn DB (Postgres) v√† Broker (Redis) ho·∫°t ƒë·ªông. |

### C. B·ªï sung D·ªãch v·ª• Airflow (ƒê·ªÉ c√≥ "Full Stack")

M√¥ h√¨nh Airflow c·ªßa b·∫°n ƒëang thi·∫øu hai d·ªãch v·ª• b·∫Øt bu·ªôc ƒë·ªÉ ho·∫°t ƒë·ªông:

1.  **`airflow-webserver`**: Cung c·∫•p giao di·ªán Web UI (ƒë√£ c√≥ port 8082:8080 trong d·ªãch v·ª• `airflow` ·ªü ph·∫ßn tr∆∞·ªõc, nh∆∞ng c·∫ßn t√°ch ra).
2.  **`airflow-scheduler`**: L·∫≠p l·ªãch v√† ki·ªÉm tra tr·∫°ng th√°i DAGs.

B·∫°n n√™n t√°ch d·ªãch v·ª• `airflow` (Web UI/Scheduler) th√†nh c√°c th√†nh ph·∫ßn ri√™ng bi·ªát n·∫øu b·∫°n ƒëang s·ª≠ d·ª•ng `LocalExecutor` ho·∫∑c `CeleryExecutor` (nh∆∞ Flower ch·ªâ ra).

---

## 3. C·∫•u h√¨nh C·∫≠p nh·∫≠t (M√¥ ph·ªèng)

T√¥i gi·∫£ ƒë·ªãnh b·∫°n mu·ªën d√πng **`spark-net`** l√†m m·∫°ng chung.


http://googleusercontent.com/immersive_entry_chip/1

### T√≥m t·∫Øt c√°c ƒëi·ªÅu ch·ªânh ch√≠nh:

1.  **S·ª≠a `nginx.conf`**: Thay th·∫ø c√°c IP c·ª©ng b·∫±ng t√™n d·ªãch v·ª• (`postgres`, `oracle-db`).
2.  **H·ª£p nh·∫•t Network**: ƒê√£ ƒë·ªïi t·∫•t c·∫£ c√°c d·ªãch v·ª• li√™n quan ƒë·∫øn Airflow/Redis t·ª´ `airflow-network` sang **`spark-net`** ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a giao ti·∫øp v·ªõi Spark, Kafka, v√† MinIO.
3.  **T√°ch Airflow**: ƒê√£ t√°ch Airflow th√†nh `airflow-webserver` v√† `airflow-scheduler` (v√† ƒë·ªïi t√™n d·ªãch v·ª• Airflow c≈© th√†nh `airflow-webserver` v√† `airflow-scheduler`) ƒë·ªÉ c√≥ m·ªôt tri·ªÉn khai production-ready h∆°n.
-------------END SETUP-------------