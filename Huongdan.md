# Spark Toturial:
```css
spark_workspace/
â”‚
â”œâ”€ environment.yml
â”œâ”€ start_spark_lab.bat      â† dÃ¹ng trÃªn Windows
â”œâ”€ start_spark_lab.sh       â† dÃ¹ng trÃªn macOS/Linux
â”‚
â”œâ”€ notebooks/               â† nÆ¡i chá»©a Jupyter notebooks
â”‚    â”œâ”€ spark_intro.ipynb
â”‚    â”œâ”€ streaming_kafka.ipynb
â”‚
â””â”€ data/                    â† data máº«u / input
        â”œâ”€ sample_data.csv
        â”œâ”€ kafka_messages.txt
```

# Táº¡o mÃ´i trÆ°á»ng tÃªn "spark_env" vá»›i Python 3.10
```bash
conda create -n spark_env python=3.10 -y
```

# KÃ­ch hoáº¡t mÃ´i trÆ°á»ng
```bash
conda activate spark_env
```

# CÃ i Spark vÃ  cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
```bash
pip install pyspark findspark jupyterlab pandas numpy pyarrow
```

# delta tables hoáº·c streaming:
```bash
pip install delta-spark kafka-python
```

# Kiá»ƒm tra cÃ i Ä‘áº·t Spark

```bash
python -c "import pyspark; print(pyspark.__version__)"
```

# Test session
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("TestSpark") \
    .getOrCreate()

print("Spark version:", spark.version)
spark.stop()
```

# Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng

```bash
export PYSPARK_PYTHON=$(which python)
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS="lab"
```


# Táº¡o biáº¿n mÃ´i trÆ°á»ng báº±ng file environment.yml:
```yaml
name: spark_env
channels:
  - conda-forge
  - defaults
dependencies:
  # --- Python core ---
  - python=3.10
  - pip
  - numpy
  - pandas
  - pyarrow
  - openjdk=11        # cáº§n cho Spark local mode (Java runtime)
  - jupyterlab
  - ipykernel
  - requests
  - tqdm
  - setuptools
  - wheel

  # --- PIP packages ---
  - pip:
      - pyspark==3.5.1
      - findspark
      - delta-spark
      - kafka-python
      - boto3
      - google-cloud-storage
      - azure-storage-blob
      - sqlalchemy
      - pyodbc
      - oracledb
      - pymysql
```

## ğŸ’¡ Ghi chÃº:

* openjdk=11: Spark 3.5 yÃªu cáº§u Java 11.
* delta-spark: náº¿u báº¡n cÃ³ dÃ¹ng Delta Lake (cho realtime/CDC).
* kafka-python: Ä‘á»ƒ káº¿t ná»‘i Apache Kafka topic.
* oracledb + pyodbc: Ä‘á»ƒ Ä‘á»c/ghi dá»¯ liá»‡u vá»›i Oracle, SQL Server.
* azure-storage-blob, google-cloud-storage, boto3: Ä‘á»ƒ Ä‘á»c ghi file trÃªn Cloud (Azure, GCP, AWS).

## CÃ¡ch sá»­ dá»¥ng file:
```makefile
D:\WorkSpace\Python\spark_toturial\environment.yml
```
## Táº¡o mÃ´i trÆ°á»ng Conda
```bash
conda env create -f environment.yml
```
## KÃ­ch hoáº¡t mÃ´i trÆ°á»ng
```bash
conda activate spark_env
```
## Kiá»ƒm tra
```bash
python -c "import pyspark; print(pyspark.__version__)"
```

# ğŸ§  3ï¸âƒ£ (Tuá»³ chá»n) Cháº¡y Spark vá»›i JupyterLab:

## Ä‘Äƒng kÃ½ kernel Ä‘á»ƒ dÃ¹ng trong Jupyter
```bash
python -m ipykernel install --user --name spark_env --display-name "Spark (PySpark 4.0.1)"
```
## khá»Ÿi Ä‘á»™ng JupyterLab
```bash
jupyter lab
```

# ğŸ”’ 4ï¸âƒ£ Báº£o máº­t & tÆ°Æ¡ng thÃ­ch (náº¿u báº¡n Ä‘ang trong mÃ´i trÆ°á»ng ngÃ¢n hÃ ng)

* Dá»¯ liá»‡u nháº¡y cáº£m â†’ nÃªn disable internet access cá»§a Conda environment (dÃ¹ng mirror ná»™i bá»™).

* Náº¿u cluster Spark dÃ¹ng Kerberos / LDAP â†’ cÃ i thÃªm:

```bash
pip install requests-kerberos pyspnego
```

* Náº¿u cáº§n giao tiáº¿p SFTP hoáº·c SSH:
```bash
pip install paramiko pysftp  
```

# Delta Lake hoáº·c Kafka Streaming
```bash
pip install "delta-spark>=3.2.0" "kafka-python>=2.0.2"
```

# Khi cháº¡y trÃªn Windows, Ä‘á»ƒ trÃ¡nh lá»—i â€œWinUtils not foundâ€, cÃ³ thá»ƒ cÃ i thÃªm gÃ³i giáº£ láº­p:
```bash
pip install winutils
```

## Window - Java báº±ng pip (cÃ¡ch dá»± phÃ²ng):

```bash
pip install jdk4spark
```
â†’ ThÆ° viá»‡n nÃ y chá»©a JRE 11 tá»‘i giáº£n, tá»± Ä‘á»™ng giáº£i nÃ©n vÃ o .local/jdk trong user folder.

Sau Ä‘Ã³ thÃªm biáº¿n:

```bat
SET JAVA_HOME=%USERPROFILE%\.local\jdk
SET PATH=%JAVA_HOME%\bin;%PATH%
```
