# üê≥ Docker + Spark + Airflow: H∆∞·ªõng d·∫´n t·ªïng h·ª£p
## 1Ô∏è‚É£ Qu·∫£n l√Ω Docker
Start / Stop containers

```bash
# Start t·∫•t c·∫£ service trong docker-compose
docker compose up -d

# Stop t·∫•t c·∫£ service
docker compose down

# Restart service
docker compose restart <service_name>

# Start 1 service ri√™ng l·∫ª
docker compose up -d <service_name>

# Stop 1 service ri√™ng l·∫ª
docker compose stop <service_name>
```

Ki·ªÉm tra tr·∫°ng th√°i

```bash
# Xem tr·∫°ng th√°i t·∫•t c·∫£ containers
docker compose ps

# Xem logs c·ªßa container
docker compose logs -f <service_name>

# Xem logs c·ªßa t·∫•t c·∫£ containers
docker compose logs -f
```

Qu·∫£n l√Ω image & container

```bash
# Li·ªát k√™ image ƒë√£ pull
docker images

# Pull 1 image m·ªõi
docker pull <image_name>

# Remove container
docker rm <container_name>

# Remove image
docker rmi <image_name>
```

## 2Ô∏è‚É£ Truy c·∫≠p c√°c service

| Service             | URL / Command                                             | Port |
| ------------------- | --------------------------------------------------------- | ---- |
| **Spark Master UI** | [http://localhost:8080](http://localhost:8080)            | 8080 |
| **Spark Worker UI** | [http://localhost:8081](http://localhost:8081)            | 8081 |
| **JupyterLab**      | [http://localhost:8888](http://localhost:8888)            | 8888 |
| **Airflow Web UI**  | [http://localhost:8082](http://localhost:8082)            | 8082 |
| **Kafka (cli)**     | `docker exec -it kafka /bin/bash` ‚Üí `kafka-topics.sh ...` | 9092 |
| **ZooKeeper (cli)** | `docker exec -it zookeeper /bin/bash` ‚Üí `zkCli.sh`        | 2181 |
| **MinIO Console**   | [http://localhost:9001](http://localhost:9001)            | 9001 |
| **Postgres (cli)**  | `docker exec -it postgres psql -U airflow -d airflow`     | 5432 |

* üîπ L∆∞u √Ω: n·∫øu d√πng Windows, b·∫°n c·∫ßn ƒë·∫£m b·∫£o c√°c c·ªïng ch∆∞a b·ªã chi·∫øm tr∆∞·ªõc ƒë√≥.

## 3Ô∏è‚É£ Demo flow Spark + Airflow

## A. Spark + PySpark trong JupyterLab

1. Truy c·∫≠p: http://localhost:8888

2. Trong notebook:

```python
import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("DemoSpark") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# T·∫°o DataFrame demo
data = [("Alice", 30), ("Bob", 25)]
columns = ["name", "age"]
df = spark.createDataFrame(data, columns)

df.show()

```

3. Ki·ªÉm tra job tr√™n Spark UI ‚Üí http://localhost:8080

## B. Airflow DAG demo

1. Truy c·∫≠p: http://localhost:8082

2. T·∫°o DAG dags/demo_spark_dag.py:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def demo_task():
    print("Hello Airflow + Spark!")

with DAG("demo_spark_dag",
         start_date=datetime(2025, 1, 1),
         schedule_interval=None,
         catchup=False) as dag:

    t1 = PythonOperator(
        task_id="hello_spark",
        python_callable=demo_task
    )
```

3. Trigger DAG ‚Üí xem log ‚Üí ki·ªÉm tra output Hello Airflow + Spark!

## C. Kafka + Spark Streaming:

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StringType

spark = SparkSession.builder \
    .appName("KafkaDemo") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "test-topic") \
    .load()

df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").writeStream \
    .format("console") \
    .start() \
    .awaitTermination()
```

## 4Ô∏è‚É£ Bonus: ch·∫°y Spark + Airflow ngo√†i Docker

N·∫øu mu·ªën ch·∫°y tr√™n Conda:

## A. Spark:

```bash
conda activate spark_env
export JAVA_HOME=<path_to_java>
export PATH=$JAVA_HOME/bin:$PATH

# Start Spark Master
$SPARK_HOME/sbin/start-master.sh
# Start Spark Worker
$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077
```

## B. Jupyter + PySpark

```bash
export PYSPARK_PYTHON=python
jupyter lab
```

## C. Airflow

```bash
# Init DB
airflow db init

# Start scheduler & webserver
airflow scheduler &
airflow webserver -p 8082

```

* üîπ L·ª£i √≠ch: kh√¥ng c·∫ßn Docker, c√≥ th·ªÉ debug tr·ª±c ti·∫øp, d√πng Python + pip.
* üîπ H·∫°n ch·∫ø: ph·∫£i c√†i ƒë·ªß Java, Spark, Hadoop, Postgres, Kafka th·ªß c√¥ng.


# Docker ki·ªÉm tra:

```bash
docker ps --format "table {{.Names}}\t{{.Status}}"
```

full

```bash
docker compose ps
```
# Docker cleanup
## üî• 1Ô∏è‚É£ Xo√° to√†n b·ªô log c·ªßa container (t·ª± ƒë·ªông gi·∫£m file JSON log)

Docker log n·∫±m ·ªü:
```
/var/lib/docker/containers/<container-id>/<container-id>-json.log
```
L·ªánh d·ªçn:
```bash
docker ps -aq | xargs -I {} sh -c 'truncate -s 0 /var/lib/docker/containers/{}/{}-json.log 2>/dev/null'
```
### ‚ö†Ô∏è Note:
Tr√™n macOS, ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø n·∫±m trong VM, nh∆∞ng Docker Desktop h·ªó tr·ª£ truncate qua CLI.

* ‚úî Log s·∫Ω tr·ªü v·ªÅ 0 byte
* ‚úî Container kh√¥ng restart
* ‚úî Kh√¥ng m·∫•t d·ªØ li·ªáu volume

## üî• 2Ô∏è‚É£ X√≥a container ƒë√£ d·ª´ng:

```bash
docker container prune -f
```

## üî• 3Ô∏è‚É£ X√≥a image kh√¥ng d√πng (dangling + orphan)

```bash
docker image prune -a -f
```
N·∫øu mu·ªën xem tr∆∞·ªõc khi xo√°:
```bash
docker image prune -a
```
## üî• 4Ô∏è‚É£ Xo√° network r√°c (docker-compose up/down nhi·ªÅu s·∫Ω sinh ra)
```bash
docker network prune -f
```
## üî• 5Ô∏è‚É£ Xo√° volume r√°c (kh√¥ng c√≤n g·∫Øn v√†o container n√†o)

```bash
docker volume prune -f
```
>‚ö†Ô∏è L∆∞u √Ω: volume prune ch·ªâ xo√° volume kh√¥ng s·ª≠ d·ª•ng ‚Üí an to√†n.

## üî• 6Ô∏è‚É£ Xo√° to√†n b·ªô build cache (r·∫•t n·∫∑ng, 2‚Äì20GB)
```bash
docker builder prune -a -f
```
## üî• 7Ô∏è‚É£ X√≥a m·ªçi th·ª© kh√¥ng d√πng (CLEAN FULL)
```bash
docker system prune -a --volumes -f
```
>### ‚ö†Ô∏è C·∫©n tr·ªçng:
>*	Xo√° t·∫•t c·∫£ container STOPPED
>*	Xo√° m·ªçi image kh√¥ng ƒë∆∞·ª£c container n√†o d√πng
>*	Xo√° network r√°c
>*	Xo√° build cache
>*	Xo√° volume kh√¥ng d√πng
>> Nh∆∞ng s·∫Ω kh√¥ng xo√° volume ƒëang mount cho project.

## üî• 8Ô∏è‚É£ Ki·ªÉm tra dung l∆∞·ª£ng Docker sau khi d·ªçn
```bash
docker system df
```
ch·∫°y l·ªánh n√†y tr∆∞·ªõc ‚Üí ƒë·ªÉ xem c√°i g√¨ ƒëang chi·∫øm dung l∆∞·ª£ng:

output v√≠ d·ª•:
```
> docker system df
TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE
Images          21        21        18.51GB   3.829GB (20%)
Containers      26        18        597.5MB   99.27MB (16%)
Local Volumes   50        7         390.2MB   321.5MB (82%)
Build Cache     55        0         2.936GB   2.936GB
```
## üî• 9Ô∏è‚É£ Docker Desktop GUI c≈©ng c√≥ n√∫t d·ªçn cache
Settings ‚Üí Troubleshoot ‚Üí Clean/Purge Data
Nh∆∞ng CLI ch√≠nh x√°c h∆°n v√† tu·ª≥ ch·ªânh ƒë∆∞·ª£c.
## ‚≠ê G·ª£i √Ω d·ªçn d·∫πp

V√¨ Project ƒëang build r·∫•t nhi·ªÅu docker image big-size (Spark, Airflow, Keycloak, Ranger, Atlas, Prometheus, Loki‚Ä¶), n√™n khuy√™n ch·∫°y:

G√≥i d·ªçn ti√™u chu·∫©n n√™n d√πng h·∫±ng ng√†y:
```bash
docker system prune -f
docker builder prune -f
docker volume prune -f
```
G√≥i d·ªçn to√†n b·ªô (1 tu·∫ßn/l·∫ßn)
```bash
docker system prune -a --volumes -f
```


# ‚úÖ T·ªïng k·∫øt

Docker gi√∫p b·∫°n ch·∫°y ƒë·∫ßy ƒë·ªß stack Spark + Hadoop + Kafka + Airflow + MinIO + Postgres ch·ªâ b·∫±ng 1 c√¢u l·ªánh.

JupyterLab + PySpark cho dev/test notebook nhanh.

Airflow qu·∫£n l√Ω task & DAG workflow.

Ngo√†i Docker, Conda + native install gi√∫p debug & develop tr·ª±c ti·∫øp.

