# üê≥ Docker + Spark + Airflow: H∆∞·ªõng d·∫´n t·ªïng h·ª£p
## 1Ô∏è‚É£ Qu·∫£n l√Ω Docker
Start / Stop containers

```bash
# Start t·∫•t c·∫£ service trong docker-compose
docker compose up -d

# Stop t·∫•t c·∫£ service
docker compose down

# Restart service
docker compose restart <service_name>

# Start 1 service ri√™ng l·∫ª
docker compose up -d <service_name>

# Stop 1 service ri√™ng l·∫ª
docker compose stop <service_name>
```

Ki·ªÉm tra tr·∫°ng th√°i

```bash
# Xem tr·∫°ng th√°i t·∫•t c·∫£ containers
docker compose ps

# Xem logs c·ªßa container
docker compose logs -f <service_name>

# Xem logs c·ªßa t·∫•t c·∫£ containers
docker compose logs -f
```

Qu·∫£n l√Ω image & container

```bash
# Li·ªát k√™ image ƒë√£ pull
docker images

# Pull 1 image m·ªõi
docker pull <image_name>

# Remove container
docker rm <container_name>

# Remove image
docker rmi <image_name>
```

## 2Ô∏è‚É£ Truy c·∫≠p c√°c service

| Service             | URL / Command                                             | Port |
| ------------------- | --------------------------------------------------------- | ---- |
| **Spark Master UI** | [http://localhost:8080](http://localhost:8080)            | 8080 |
| **Spark Worker UI** | [http://localhost:8081](http://localhost:8081)            | 8081 |
| **JupyterLab**      | [http://localhost:8888](http://localhost:8888)            | 8888 |
| **Airflow Web UI**  | [http://localhost:8082](http://localhost:8082)            | 8082 |
| **Kafka (cli)**     | `docker exec -it kafka /bin/bash` ‚Üí `kafka-topics.sh ...` | 9092 |
| **ZooKeeper (cli)** | `docker exec -it zookeeper /bin/bash` ‚Üí `zkCli.sh`        | 2181 |
| **MinIO Console**   | [http://localhost:9001](http://localhost:9001)            | 9001 |
| **Postgres (cli)**  | `docker exec -it postgres psql -U airflow -d airflow`     | 5432 |

* üîπ L∆∞u √Ω: n·∫øu d√πng Windows, b·∫°n c·∫ßn ƒë·∫£m b·∫£o c√°c c·ªïng ch∆∞a b·ªã chi·∫øm tr∆∞·ªõc ƒë√≥.

## 3Ô∏è‚É£ Demo flow Spark + Airflow

## A. Spark + PySpark trong JupyterLab

1. Truy c·∫≠p: http://localhost:8888

2. Trong notebook:

```python
import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("DemoSpark") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# T·∫°o DataFrame demo
data = [("Alice", 30), ("Bob", 25)]
columns = ["name", "age"]
df = spark.createDataFrame(data, columns)

df.show()

```

3. Ki·ªÉm tra job tr√™n Spark UI ‚Üí http://localhost:8080

## B. Airflow DAG demo

1. Truy c·∫≠p: http://localhost:8082

2. T·∫°o DAG dags/demo_spark_dag.py:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def demo_task():
    print("Hello Airflow + Spark!")

with DAG("demo_spark_dag",
         start_date=datetime(2025, 1, 1),
         schedule_interval=None,
         catchup=False) as dag:

    t1 = PythonOperator(
        task_id="hello_spark",
        python_callable=demo_task
    )
```

3. Trigger DAG ‚Üí xem log ‚Üí ki·ªÉm tra output Hello Airflow + Spark!

## C. Kafka + Spark Streaming:

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StringType

spark = SparkSession.builder \
    .appName("KafkaDemo") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "test-topic") \
    .load()

df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").writeStream \
    .format("console") \
    .start() \
    .awaitTermination()
```

## 4Ô∏è‚É£ Bonus: ch·∫°y Spark + Airflow ngo√†i Docker

N·∫øu mu·ªën ch·∫°y tr√™n Conda:

## A. Spark:

```bash
conda activate spark_env
export JAVA_HOME=<path_to_java>
export PATH=$JAVA_HOME/bin:$PATH

# Start Spark Master
$SPARK_HOME/sbin/start-master.sh
# Start Spark Worker
$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077
```

## B. Jupyter + PySpark

```bash
export PYSPARK_PYTHON=python
jupyter lab
```

## C. Airflow

```bash
# Init DB
airflow db init

# Start scheduler & webserver
airflow scheduler &
airflow webserver -p 8082

```

* üîπ L·ª£i √≠ch: kh√¥ng c·∫ßn Docker, c√≥ th·ªÉ debug tr·ª±c ti·∫øp, d√πng Python + pip.
* üîπ H·∫°n ch·∫ø: ph·∫£i c√†i ƒë·ªß Java, Spark, Hadoop, Postgres, Kafka th·ªß c√¥ng.

# ‚úÖ T·ªïng k·∫øt

Docker gi√∫p b·∫°n ch·∫°y ƒë·∫ßy ƒë·ªß stack Spark + Hadoop + Kafka + Airflow + MinIO + Postgres ch·ªâ b·∫±ng 1 c√¢u l·ªánh.

JupyterLab + PySpark cho dev/test notebook nhanh.

Airflow qu·∫£n l√Ω task & DAG workflow.

Ngo√†i Docker, Conda + native install gi√∫p debug & develop tr·ª±c ti·∫øp.

