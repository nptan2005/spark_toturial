# üß± I. Docker Compose ho√†n ch·ªânh (docker-compose.yml)

```yaml
version: "3.9"

services:
  # ------------------- SPARK -------------------
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_PUBLIC_DNS=spark-master
    networks:
      - spark-net

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"
    networks:
      - spark-net

  # ------------------- KAFKA -------------------
  zookeeper:
    image: bitnami/zookeeper:3.8
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    networks:
      - spark-net

  kafka:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - spark-net

  # ------------------- MINIO (Data Lake) -------------------
  minio:
    image: minio/minio
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=admin123
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio:/data
    networks:
      - spark-net

  # ------------------- POSTGRES (Metadata / Airflow DB) -------------------
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - spark-net

  # ------------------- AIRFLOW -------------------
  airflow:
    image: apache/airflow:2.10.5-python3.12
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=C1Cg8QaV6rUzSZlQ9OCAFHVv-IWMBQuSvnfcMfnuEAg=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    ports:
      - "8082:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    depends_on:
      - postgres
    networks:
      - spark-net

  # ------------------- JUPYTER -------------------
  jupyterlab:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyterlab
    ports:
      - "8888:8888"
    volumes:
      - ./data:/home/jovyan/data
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge

```

## C√°ch ch·∫°y:

```bash
# Kh·ªüi ƒë·ªông to√†n b·ªô stack
docker compose up -d

# Xem log
docker compose logs -f airflow
docker compose logs -f spark-master

# D·ª´ng to√†n b·ªô
docker compose down

```

## Access:

| Service      | URL                                            | Ghi ch√∫                   |
| ------------ | ---------------------------------------------- | ------------------------- |
| Spark Master | [http://localhost:8080](http://localhost:8080) | UI cluster Spark          |
| Spark Worker | [http://localhost:8081](http://localhost:8081) | Worker UI                 |
| JupyterLab   | [http://localhost:8888](http://localhost:8888) | Notebook dev              |
| Airflow      | [http://localhost:8082](http://localhost:8082) | UI qu·∫£n l√Ω DAG            |
| MinIO        | [http://localhost:9001](http://localhost:9001) | Giao di·ªán qu·∫£n l√Ω file S3 |
| Kafka        | localhost:9092                                 | D√πng producer/consumer    |
| Postgres     | localhost:5432                                 | Database metadata         |

## G·ª£i √Ω DAG m·∫´u Airflow ƒë·ªÉ ch·∫°y Spark Job:

```python
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime

with DAG(
    dag_id="spark_wordcount_demo",
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:

    wordcount = SparkSubmitOperator(
        task_id="run_wordcount",
        application="/opt/airflow/dags/jobs/wordcount.py",
        conn_id="spark_default",
        executor_memory="2g",
        driver_memory="2g",
        verbose=True,
    )

```

# üöÄ II. H∆∞·ªõng d·∫´n kh·ªüi ƒë·ªông m√¥i tr∆∞·ªùng
## 1Ô∏è‚É£ T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c
```bash
mkdir -p spark_env/{data,notebooks,postgres_data,minio_data}
cd spark_env
```

Sao ch√©p file docker-compose.yml v√†o th∆∞ m·ª•c n√†y.

## 2Ô∏è‚É£ Kh·ªüi ƒë·ªông to√†n b·ªô cluster
```bash
docker compose up -d
```

‚è±Ô∏è Qu√° tr√¨nh kh·ªüi ƒë·ªông m·∫•t kho·∫£ng 1‚Äì2 ph√∫t cho l·∫ßn ƒë·∫ßu (t·∫£i image).

## 3Ô∏è‚É£ Ki·ªÉm tra tr·∫°ng th√°i container
```bash
docker ps
```

B·∫°n s·∫Ω th·∫•y danh s√°ch container ƒëang ch·∫°y:

| Container    | Purpose             | Port                  |
| ------------ | ------------------- | --------------------- |
| spark-master | Spark Master Web UI | 8080                  |
| spark-worker | Spark Worker Web UI | 8081                  |
| jupyterlab   | JupyterLab Notebook | 8888                  |
| kafka        | Message Broker      | 9092                  |
| zookeeper    | Kafka dependency    | 2181                  |
| postgres     | Metadata / ETL      | 5432                  |
| minio        | Object Storage      | 9000 (API), 9001 (UI) |


# üß† III. C√°ch truy c·∫≠p & test nhanh
## üîπ Spark Master UI

üëâ http://localhost:8080

## üîπ JupyterLab Notebook

üëâ http://localhost:8888

## üîπ MinIO Web Console

üëâ http://localhost:9001

```text
ƒêƒÉng nh·∫≠p:

Username: admin  
Password: admin123
```

## üîπ PostgreSQL

K·∫øt n·ªëi qua DBeaver, Azure Data Studio, ho·∫∑c psql:

```text
Host: localhost
Port: 5432
User: sparkuser
Password: sparkpass
Database: sparkdb
```

# üí° IV. Gi·∫£i th√≠ch t·ª´ng c·∫•u ph·∫ßn
## Th√†nh ph·∫ßn	Vai tr√≤	Ghi ch√∫
| Th√†nh ph·∫ßn                | Vai tr√≤                                    | Ghi ch√∫                                     |
| ------------------------- | ------------------------------------------ | ------------------------------------------- |
| **Spark Master / Worker** | X·ª≠ l√Ω batch, ETL, streaming                | C√≥ th·ªÉ scale th√™m worker                    |
| **JupyterLab**            | Giao di·ªán ph√°t tri·ªÉn & ch·∫°y notebook Spark | C√≥ s·∫µn pyspark, pandas, scikit-learn        |
| **Kafka / Zookeeper**     | Thu th·∫≠p & ph√°t realtime data stream       | D√πng cho CDC, event-driven ETL              |
| **PostgreSQL**            | Metadata Store / Transaction logs          | D√πng cho Airflow, Spark checkpoint          |
| **MinIO**                 | Object storage (nh∆∞ S3)                    | D√πng ƒë·ªÉ l∆∞u Delta Table ho·∫∑c output parquet |
| **spark-net**             | Docker network bridge                      | Gi√∫p c√°c service truy c·∫≠p l·∫´n nhau          |


# üß∞ V. M·ªôt s·ªë l·ªánh h·ªØu √≠ch
D·ª´ng to√†n b·ªô container
```bash
docker compose down
```

## X√≥a to√†n b·ªô container + volume
```bash
docker compose down -v
```
## Xem log c·ªßa t·ª´ng service
```bash
docker logs -f spark-master
```
#üìó VI. T√πy ch·ªçn m·ªü r·ªông (cho ng√¢n h√†ng)

B·∫°n c√≥ th·ªÉ d·ªÖ d√†ng b·ªï sung th√™m:

Airflow (ETL orchestration)

Ranger + Atlas (Governance & Data Masking)

Keycloak (SSO + IAM cho Jupyter / Spark UI)

#üì¶ VII. K·∫øt h·ª£p v·ªõi Conda Local (Dual Mode)

B·∫°n ho√†n to√†n c√≥ th·ªÉ ch·∫°y:

Local Mode: Conda env (spark_env)

Cluster Mode: Docker Compose (multi-node Spark, Kafka, MinIO)

Hai m√¥i tr∆∞·ªùng n√†y c√≥ th·ªÉ chia s·∫ª chung th∆∞ m·ª•c data/ v√† notebooks/.

# üåê 5. Truy c·∫≠p giao di·ªán:

| Service      | URL                                            | Ghi ch√∫                   |
| ------------ | ---------------------------------------------- | ------------------------- |
| Spark Master | [http://localhost:8080](http://localhost:8080) | UI cluster Spark          |
| Spark Worker | [http://localhost:8081](http://localhost:8081) | Worker UI                 |
| JupyterLab   | [http://localhost:8888](http://localhost:8888) | Notebook dev              |
| Airflow      | [http://localhost:8082](http://localhost:8082) | UI qu·∫£n l√Ω DAG            |
| MinIO        | [http://localhost:9001](http://localhost:9001) | Giao di·ªán qu·∫£n l√Ω file S3 |
| Kafka        | localhost:9092                                 | D√πng producer/consumer    |
| Postgres     | localhost:5432                                 | Database metadata         |


# üß© 6. G·ª£i √Ω DAG m·∫´u Airflow ƒë·ªÉ ch·∫°y Spark Job:

```python
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime

with DAG(
    dag_id="spark_wordcount_demo",
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:

    wordcount = SparkSubmitOperator(
        task_id="run_wordcount",
        application="/opt/airflow/dags/jobs/wordcount.py",
        conn_id="spark_default",
        executor_memory="2g",
        driver_memory="2g",
        verbose=True,
    )

```

# üíæ 7. H∆∞·ªõng d·∫´n t·∫°o file .env cho Docker Compose

```env
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=admin123
AIRFLOW_UID=50000
AIRFLOW_GID=0
FERNET_KEY=C1Cg8QaV6rUzSZlQ9OCAFHVv-IWMBQuSvnfcMfnuEAg=

```

